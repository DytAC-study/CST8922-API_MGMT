# Demo Architecture: Kong API Gateway with OAuth2 Proxy for API Protection and Auth

## Introduction

In modern microservice deployments, an **API Gateway** acts as a central entry point to enforce security, rate limiting, and monitoring across APIs. This report explains a demo architecture using **Kong** (an open-source API gateway) together with **OAuth2 Proxy** (for GitHub OAuth authentication) to protect a backend API. We describe how the system behaves and is configured in a local Minikube Kubernetes cluster versus in Azure Kubernetes Service (AKS), clarify the roles of each component (Kong, OAuth2 Proxy, backend API), and detail key configuration settings. We also overview **JWT authentication** (signing and validation) and how it relates to Kong and OAuth2 Proxy, including whether both can be combined. Finally, we briefly compare Kong to **Tyk** (another API gateway) in terms of plugin support, policy enforcement, authentication features, scalability, and monitoring.

## Architecture and Components Overview

**Components in the Demo Architecture:**

- **Kong API Gateway (OSS)** – Deployed as an Ingress Controller in Kubernetes, Kong routes client requests to services and enforces policies like authentication and rate limiting. It sits at the cluster’s edge, exposing APIs to clients and forwarding requests to the backend[techtarget.com](https://www.techtarget.com/searchapparchitecture/tip/API-gateway-comparison-Kong-vs-Tyk#:~:text=Kong uses servers and a,plugins at the back end). Kong is highly extensible via plugins (e.g. auth, logging, rate limiting) and can run with or without a database for config storage[cloud.theodo.com](https://cloud.theodo.com/en/blog/kong-apigateway-kubernetes#:~:text=manage all API gateway routes).
- **OAuth2 Proxy** – A dedicated authentication proxy that handles OAuth 2.0 login flows (in this case, with GitHub as the identity provider) and manages user sessions via cookies. It runs internally and ensures that only authenticated requests reach the backend. OAuth2 Proxy simplifies adding OAuth/OIDC login to apps without modifying the app code[developer.okta.com](https://developer.okta.com/blog/2022/07/14/add-auth-to-any-app-with-oauth2-proxy#:~:text=OAuth2 Proxy is a reverse,application have already been authorized).
- **Backend API (Flask app)** – A simple application (e.g. a Python Flask service) that provides some API endpoint (like `/api/hello`). In our demo, it simply returns a greeting and echoes user info from headers. The backend is deployed in the cluster behind the gateway, and it trusts Kong/OAuth2 Proxy to handle auth (so the app can focus on core logic).
- **Kubernetes (Minikube or AKS)** – The orchestration platform hosting all components. **Minikube** is used for local testing (simulating a cluster on a developer’s machine), while **AKS** is a managed cloud Kubernetes service for production. The architecture is similar on both, with minor differences in how services are exposed and configured.
- **Client & Testing Tools** – We use **Postman** (or a web browser) to simulate client requests to the API. Postman helps in sending HTTP requests with tokens/cookies and observing responses for monitoring and tracing the request flow.

**High-Level Request Flow:**

When a client (e.g. Postman or a user’s browser) calls a protected API endpoint, the request first hits **Kong Gateway**. Kong applies any global or route-specific policies – for example, checking authentication (JWT token) or rate limits – before routing the request. In this setup, Kong forwards the request to the **OAuth2 Proxy** service (which is configured as the upstream for the protected API). The OAuth2 Proxy will ensure the user is authenticated: if no valid session is present, it triggers an OAuth login with GitHub; if the user **is** authenticated (cookie present), it adds user identity info to headers and proxies the request to the **backend API**. The backend API then returns the response (optionally including some of that user info) which bubbles back through Kong to the client. Kong can also log the request or apply response transformations as configured.

In summary:

- **Kong** – Handles external exposure, routing, and enforcement of policies (auth, rate limiting, etc.).
- **OAuth2 Proxy** – Offloads the heavy lifting of user authentication (OAuth login flow with GitHub) and maintains session state via a secure cookie.
- **Backend API** – Protected behind Kong and OAuth2 Proxy, it receives requests only after they pass through those layers.

Next, we’ll delve into how this is set up in Kubernetes (both locally and on AKS), and then examine each aspect in detail: Kong’s operation and config, the OAuth2 Proxy’s GitHub integration, JWT authentication mechanism, and how we monitor and rate-limit the API.

## Kong in Kubernetes: Deployment on Minikube vs. AKS

Kong can operate as a Kubernetes **Ingress Controller**, meaning it dynamically configures itself based on Kubernetes Ingress resources and custom resources. In both Minikube and AKS, we deploy Kong in the cluster to serve as the API gateway. The core behavior is the same, but there are a few differences in setup:

- **Deployment & Ingress Class:** Kong is typically installed via the official Helm chart or YAML manifests. This sets up the **Kong Ingress Controller** (often a Deployment) along with the Kong proxy itself. In Kubernetes, Kong will watch for Ingress resources labeled with a specific class (e.g. `ingressClassName: kong`) and translate them into Kong routes/services internally. In our demo, we create an Ingress resource for the API route(s) we want Kong to handle. Kong’s controller ensures that any Ingress or custom resource (like KongPlugin, KongConsumer) is applied to the running gateway configuration.
- **Service Type (Exposure):** On **Minikube**, there is no built-in cloud Load Balancer. We often expose Kong via a NodePort service or use Minikube’s tunnel/Ingress addons. For example, Kong’s proxy can be mapped to a localhost port or the Minikube VM’s IP. In our local setup, we might simply port-forward to Kong or use `minikube service` to access it. On **AKS**, Kong is typically exposed with a Service of type `LoadBalancer`, which provisions an Azure Load Balancer IP for external access. This means in AKS, Kong gets a public endpoint (IP or DNS) reachable from the internet from outside). We would configure our DNS or client to call that address. The Ingress resource’s host (e.g. `api.example.com`) can be mapped to this LB.
- **Environment Configuration:** In both environments, Kong requires some configuration via environment variables or ConfigMap. For instance:
  - `KONG_DATABASE` can be set to `postgres` or `off` depending on whether we use a database. In a quick demo, **DB-less mode** (declarative config in a ConfigMap) is convenient, but in our described setup we used Kong’s admin API with a database to dynamically add plugins/consumers. Kong supports both approaches. In Minikube, one might use DB-less for simplicity (since everything resets easily), whereas in AKS a Postgres instance (in-cluster or external) could be used for persistence of Kong config across restarts.
  - `KONG_PROXY_LISTEN` and `KONG_ADMIN_LISTEN` configure Kong’s ports. By default, the proxy listens on 80/8000 (HTTP) and 443/8443 (HTTPS) for client traffic, and the admin API on 8001 (only inside cluster). We typically leave these default. In AKS behind a LoadBalancer, the LB will forward to Kong’s proxy ports.
  - `KONG_PLUGINS` can list any custom plugins to load. For our needs, the standard plugins (rate-limiting, JWT, etc.) are already bundled with Kong.
- **Ingress and Routing Configuration:** We define a Kubernetes **Ingress** resource to tell Kong how to route requests. For example, we might create an Ingress for path `/api/` (or a host like `demo.example.com`) that points to the **OAuth2 Proxy service** in the cluster. Kong’s controller will convert this to a Kong **Route** and **Service** internally. The target Service would be OAuth2 Proxy (which itself will forward to the actual backend). We also likely create another Ingress (or an additional path) for the OAuth2 Proxy’s own endpoints (`/oauth2/*`), so that the callbacks and login routes are accessible. In Minikube, our ingress might use a dummy host (or `localhost`) and we rely on port-forwarding; in AKS, we use a real hostname with DNS pointing to the LB.
- **TLS (HTTPS):** On a local cluster, we might not configure TLS for simplicity (using HTTP and `cookie-secure=false` for OAuth2 Proxy as noted later). On AKS, it’s good practice to use HTTPS. Kong can terminate TLS at the gateway by adding a Kubernetes TLS secret and referencing it in the Ingress. For a classroom demo, this may be an advanced detail, but in a real scenario the GitHub OAuth app would require the redirect URI to match the scheme (http vs https) that the user is redirected to. (For our demo, we used HTTP for ease of setup; in production AKS, we’d use HTTPS and secure cookies).

Despite these differences, **the core behavior remains**: Kong is the first contact for clients, and it routes traffic according to rules, applying plugins for auth and rate limiting as configured. On Minikube, you’ll interact with Kong via a local address, whereas on AKS you’ll have a cloud endpoint – but Kong itself works the same way in both environments.

## Kong Gateway: Ingress Controller Behavior and Configuration

Once Kong is running in the cluster, we need to configure it to protect and forward our API. Let’s break down the key **Kong concepts and settings** in this architecture:

- **Routes and Services:** In Kong’s model, a **Service** object represents an upstream API (an internal backend URL), and a **Route** object maps client requests to that service based on criteria (path, host, etc.). In our setup, one Service will correspond to the OAuth2 Proxy (since all traffic goes through it), and the Route will match the desired paths (e.g. `/api/hello` and OAuth2 Proxy’s own `/oauth2/` URLs). For example, we might configure a Route with path `/api/` that maps to the Service pointing at `oauth2-proxy:4180` (assuming the OAuth2 Proxy service is on port 4180). Kong will then forward any requests starting with `/api` to the OAuth2 Proxy. Kong can strip path prefixes or pass them as-is; in this case we likely keep the path so OAuth2 Proxy knows what downstream path to request. *(If we had multiple distinct backend services, we could define multiple Services/Routes, but in this demo the OAuth2 Proxy fronts the single Flask API.)*
- **Plugins (Auth, Rate Limiting, etc.):** Kong **Plugins** are modular policies we can enable on a global, service, or route level. Major plugins used in this architecture:
  - **Rate Limiting Plugin:** We apply rate limiting to protect the API from abuse or accidental overload. For example, a global or route-specific rate limit might allow, say, 5 requests per minute from a client. In Kong, this is configured with plugin settings like `config.minute=5` and a `config.policy` for how counters are stored. Using `policy=local` means the counter is in-memory per Kong node (fine for a single-node demo)file-vlrsohrq3l3b3xhshqpdxufile-vlrsohrq3l3b3xhshqpdxu, whereas `policy=cluster` would use the database, or `redis` could use an external Redis for synchronized counters across multiple gateway pods. When the limit is exceeded, Kong responds with **HTTP 429 Too Many Requests**, including headers like `X-RateLimit-Limit` and `X-RateLimit-Remaining` to inform the client of the quotafile-vlrsohrq3l3b3xhshqpdxu. In our demo, Kong would enforce this limit regardless of who the user is (unless we scope it per consumer; more on that below).
  - **JWT Authentication Plugin:** Kong’s **JWT plugin** can verify JSON Web Tokens on inbound requests. If a client presents a valid JWT (in the Authorization header, cookie, or query param), Kong will cryptographically verify the token’s signature (supporting HS256 and RS256 algorithms by default). If the token is valid (and not expired), Kong allows the request through; if not, it returns a **401 Unauthorized**. Kong’s JWT plugin requires setting up **credentials** (public keys or shared secrets) associated with **Consumers** (representing API clients or users). For example, we could create a Kong Consumer for each user or client application, and provision a JWT credential (with a secret or RSA public key) for them. The JWT plugin then knows how to validate that consumer’s tokens. In this demo, however, we opted to handle authentication via OAuth2 Proxy (with cookies), so we did **not** enable Kong’s JWT plugin for the protected route. (We will discuss combining OAuth2 Proxy and JWT later.)
  - **Key Authentication (API keys)**, **Basic Auth**, etc.: Kong also offers other auth plugins (key-auth, basic-auth, OAuth2 plugin, etc.), but we focus on JWT and the external OAuth2 Proxy approach here. (For reference, Kong’s OAuth2 plugin can issue OAuth tokens itself, but that is more complex and not used since we leverage GitHub OAuth via OAuth2 Proxy.)
- **Consumers:** A **Consumer** in Kong is an entity representing an API consumer (a user or client app). Consumers are often used in conjunction with auth plugins to tie requests to a known identity. For instance, when using the JWT plugin, each token is associated with a Consumer (by embedding the consumer’s credential key/ID in the JWT claims or via the credential used to sign it). In our architecture, since we bypass Kong’s native auth in favor of OAuth2 Proxy, Kong does not automatically know the end-user’s identity – it just sees traffic coming from OAuth2 Proxy as the “upstream”. We could still create a Consumer to represent “authenticated users” and use Kong’s **auth** plugins or logging for that user, but by default Kong won’t map the OAuth2 Proxy session to a Kong Consumer. In our demo steps, we created a Consumer largely for demonstration and logging purposes (and it would be required if we later wanted to attach per-consumer rate limits or use the JWT plugin)file-vlrsohrq3l3b3xhshqpdxu. In practice, when using an external auth proxy, the per-user logic is often handled outside Kong. However, Kong’s consumer concept could be leveraged if we, say, passed a JWT (with user info) that Kong can validate – then Kong could treat each user as a Consumer (with a token mapping). This wasn’t done in our simple setup.
- **Kong as an Ingress Gateway:** Operating inside Kubernetes, Kong’s ingress controller will automatically translate K8s resources:
  - An Ingress with host/path becomes a Kong Route and Service.
  - We can attach plugins to Ingresses via annotations or by referencing KongPlugin CRDs. For example, adding an annotation `konghq.com/plugins: <plugin-name>` on an Ingress will apply that plugin on the route. In our case, we could define a `KongPlugin` CRD for rate-limiting (and another for JWT auth if used) and then attach it to the Ingress for the API route. This way, Kong knows to enforce those plugins on that route. (Alternatively, with DB-mode Kong, we could use the Admin API or declarative config to configure the plugins – our demo used the Admin API to add them post-deployment for illustration.)
  - The Kong ingress controller also defines CRDs like `KongIngress` for advanced routing config (e.g., methods allowed, custom timeouts) and `KongConsumer` as mentioned. We kept things relatively standard, relying on basic Ingress definitions.

In summary, **Kong inside Kubernetes** acts as both the ingress (replacing something like Nginx ingress) and the API gateway. It listens on the cluster’s external interface for requests, matches them to configured routes, and applies plugins (rate limiting, auth) and then forwards to the appropriate service (OAuth2 Proxy or directly to backend as configured). For our demo, Kong’s key configuration included setting up the route to point to OAuth2 Proxy, enabling rate limiting on that route, and ensuring Kong was aware of (or ignoring) auth as needed. Kong provides a central point to monitor and adjust these rules for both local and AKS deployments.

## OAuth2 Proxy for GitHub Authentication

To handle user authentication, we integrate **OAuth2 Proxy** into this architecture. OAuth2 Proxy (oauth2-proxy) is a lightweight reverse proxy that handles the OAuth 2.0/OpenID Connect login flow with an Identity Provider – in our case, GitHub. It allows us to protect web services by requiring login via an external provider (GitHub) without modifying the backend service. Here’s how it works and how we configure it:

- **Placement and Role in Flow:** In our cluster, OAuth2 Proxy runs as a separate Deployment (with a ClusterIP Service). Kong is configured to route API requests to OAuth2 Proxy, effectively interposing it between clients and the backend API. When a user makes a request to the API (through Kong), they are actually talking to OAuth2 Proxy first. OAuth2 Proxy will *authenticate* the request and then forward it to the real backend if the user is logged in. If not logged in, OAuth2 Proxy **initiates the GitHub OAuth flow**: it responds with a redirect to GitHub’s authorization URL. This redirect goes back through Kong to the user’s browser (or client). For a web browser user, this means you suddenly get GitHub’s login page.
- **GitHub OAuth2 Flow:** We have registered an OAuth application in GitHub (providing a Client ID and Client Secret). The OAuth2 Proxy is configured with those credentials. Key settings in the **OAuth2 Proxy config** (often done via command-line args or environment variables):
  - `--provider=github` tells OAuth2 Proxy to use GitHub as the OAuth provider. It knows GitHub’s OAuth endpoints (authorize and token URLs).
  - `--client-id` and `--client-secret` (or corresponding environment vars) are set to the values from our GitHub OAuth app. These allow OAuth2 Proxy to act as an OAuth client for authentication.
  - `--redirect-url` is the URL where GitHub will send the user back after successful auth. In our local demo, we used `http://localhost:4180/oauth2/callback`file-vlrsohrq3l3b3xhshqpdxu because we accessed OAuth2 Proxy directly on that port. In a production/AKS scenario, this would be something like `https://<my-domain>/oauth2/callback` (which Kong would route to the proxy). It must match exactly what’s registered in GitHub. This URL is essentially an endpoint on OAuth2 Proxy that completes the OAuth handshake.
  - `--cookie-secret` is a random secret string used by OAuth2 Proxy to encrypt its session cookiefile-vlrsohrq3l3b3xhshqpdxu. After a user logs in via GitHub, OAuth2 Proxy creates a session (to avoid needing to go to GitHub for every request). It stores session info (including tokens or identity info) in a secure cookie. The cookie is encrypted/signed with this secret so that it can’t be tampered with. In our demo, we provided a dummy secret (e.g., a 32-byte base64 string) for testing.
  - `--cookie-name` (optional) can set a custom name for the session cookie (default is something like `_oauth2_proxy`).
  - `--cookie-secure` flag: we set this to `false` in local testing so that the cookie can be transmitted over plain HTTPfile-vlrsohrq3l3b3xhshqpdxu. In AKS with TLS, we’d set it to true to ensure the cookie is only sent over HTTPS.
  - `--cookie-domain` or `--cookie-path`: not strictly needed unless we want to scope the cookie. If Kong and OAuth2 Proxy are on the same root domain, we ensure the cookie domain covers our app’s domain.
  - `--email-domain` or GitHub org/team restrictions: OAuth2 Proxy can restrict who can log in. For example, `--github-org=myOrg` could ensure only members of *myOrg* on GitHub are allowed. In a classroom demo, we might skip this, allowing any GitHub user, or limit by email domain if using Google, etc.
  - `--upstream` is used to specify the final upstream service that OAuth2 Proxy should forward to after auth. In our case, `--upstream=http://flask-api:80/` (the internal URL of our Flask service) would be set. This means OAuth2 Proxy, after verifying the user, will proxy the request to that upstream. We can have multiple upstreams or use it as a catch-all. In the config file `oauth2-proxy.cfg`, we likely define the upstream and other settings.
  - `--http-address=0.0.0.0:4180` simply tells it which port to listen on (4180 in our case, the default).
- **Headers and Cookies Passed to Backend:** Once a user successfully authenticates via GitHub (OAuth2 Proxy receives a valid OAuth authorization code, exchanges it for an access token and possibly an ID token from GitHub), it will establish a session. The default mode is that OAuth2 Proxy will set a cookie in the user’s browser to track the session. For each subsequent request, the browser sends this cookie, and OAuth2 Proxy knows the user is already logged in (it decrypts the cookie to get the session details). Now, when proxying the request to the backend API, OAuth2 Proxy adds helpful headers with the user’s identity:
  - `X-Forwarded-User` – the username or login of the user (for GitHub, this would be their GitHub username).
  - `X-Forwarded-Email` – the user’s email address (if available from the OAuth scope).
  - `X-Forwarded-Preferred-Username` or other OIDC claims depending on provider.
  - `Authorization` header – **if** we enable the option `--set-authorization-header=true`, OAuth2 Proxy will pass the OAuth access token in an `Authorization: Bearer <token>` header to the backendfile-vlrsohrq3l3b3xhshqpdxu. In our config, we did set this. With GitHub as the provider, this token is the GitHub API token (not a JWT). This could be useful if the backend wants to call the GitHub API on the user’s behalf or validate their org membership, etc. If the provider were OpenID Connect (like Google), this could also be a JWT ID token.
  - We also enabled `--pass-access-token` or similar in some setups to forward the raw token separately, but the authorization header already covers that. (There are related flags like `--pass-basic-auth`, `--pass-user-headers` which control what info is forwarded. By default, OAuth2 Proxy will pass X-Forwarded-* headers for user and email.)

Our Flask backend is written to read these headers. For example, it might do: `user = request.headers.get("X-Forwarded-User")` and then include that in the response. In fact, our demo’s Flask app returns a JSON with the `"user"` and `"email"` it sees in the headersfile-vlrsohrq3l3b3xhshqpdxu. When everything is set up, after logging in, a request to `/api/hello` might return something like: `{"message": "Hello from backend API", "user": "octocat", "email": "octocat@github.com"}` – confirming that the OAuth2 Proxy successfully authenticated the user and injected their identity.

- **OAuth2 Proxy Integration with Kong:** There are a couple of ways Kong and OAuth2 Proxy integrate:
  1. **Routing integration:** As mentioned, Kong simply routes certain paths to the OAuth2 Proxy service. For example, Kong can route all `/oauth2/*` URLs to OAuth2 Proxy (for the login, callback, and auth endpoints), as well as the main `/api/*` to OAuth2 Proxy. Essentially Kong is oblivious to the auth flow; it’s just forwarding the requests to the correct internal service. This is straightforward: we set up Ingress or Kong Routes such that:
     - `^/oauth2/` -> OAuth2 Proxy service (this covers `/oauth2/start` (the login initializer), `/oauth2/callback` (GitHub redirect hits this), and `/oauth2/auth` (used in Nginx auth_request setups), etc.).
     - `^/api/` -> OAuth2 Proxy service (which internally will forward to actual API if authenticated).
        Kong doesn’t need any special plugin in this case – it’s just passing through. The downside is Kong doesn’t itself know if a user is authorized or not, it relies entirely on OAuth2 Proxy to allow or deny requests (OAuth2 Proxy will only forward to upstream if session is valid; if not, it returns a 403 or redirect).
  2. **(Alternative) External auth plugin:** Some API gateways support an external auth hook. Kong open-source doesn’t have a direct equivalent of Nginx’s `auth_request` out of the box (though Kong Enterprise has an OpenID Connect plugin, and one could write a custom plugin). In our case, we did not use a Kong plugin for OIDC; we simply used routing as above. (There are community plugins or the Curity “Kong OAuth Proxy” plugin that can decrypt the cookie and validate tokens at the gateway, but those are beyond our scope.)
- **User Experience:** Putting it together, the first time a user tries to access the API:
  - They call the API URL -> Kong -> OAuth2 Proxy.
  - OAuth2 Proxy sees no session cookie and replies (via Kong) with a **302 Redirect** to GitHub’s OAuth site. (If using Postman, you would catch this redirect; in a browser, it happens automatically.)
  - The user logs into GitHub and approves the OAuth request. GitHub then redirects back to our `oauth2/callback` endpoint (which travels to Kong and then to OAuth2 Proxy).
  - OAuth2 Proxy validates the GitHub response, creates a session cookie, and then usually redirects the user to the original URL they wanted (this is handled via a query param `?rd=<original_url>` in the initial redirect).
  - The user’s browser, now with the session cookie set, makes the request again (or is automatically redirected) to the original API endpoint. This time, OAuth2 Proxy finds the valid cookie, so it forwards the request to the Flask API, adding `X-Forwarded-User` etc.
  - The backend returns the content, which goes back to the user. From this point on, as long as the session cookie is valid, the user can call the API and OAuth2 Proxy will directly forward the requests (no more GitHub redirects) – essentially single sign-on for the session duration.
- **Session Duration and Refresh:** By default, the cookie might last some hours. There are options like `--cookie-refresh` to periodically force a re-check with the IdP, or `--cookie-expire`. We won’t dive deep here, but know that eventually the user might need to re-login after expiry.

The **major OAuth2 Proxy configuration settings** we covered (and their purpose) can be summarized:

- `--provider`, `--client-id`, `--client-secret` – define the OAuth provider and credentials (GitHub OAuth app info).
- `--redirect-url` – where to return after login (must match app and route to the proxy).
- `--upstream` – the backend API endpoint(s) to forward to after auth.
- Session cookie settings (`--cookie-secret`, name, secure flag, domain) – secure persistence of login state.
- Headers to pass (`--set-authorization-header`, `--pass-*` flags) – how user info or tokens are forwarded to upstream.
- Allowed users (`--github-org`/`--email-domain` etc.) – to restrict access if desired.

## JWT Authentication: Signing and Validation Basics

While our demo uses OAuth2 Proxy with cookies for authentication, it’s important to understand **JWT (JSON Web Token) authentication** since Kong and Tyk both support it, and it’s a common method for API auth. Here’s a brief overview of JWT and how Kong deals with JWTs:

- **What is a JWT?** JSON Web Token (RFC 7519) is a compact, URL-safe means of representing claims between two parties. A JWT typically has three parts: a header, a payload, and a signature – encoded as Base64URL and joined by dots. For example: `header.payload.signature`. The header specifies the signing algorithm (e.g. `alg: HS256` or `RS256`) and token type. The payload contains claims like issuer (`iss`), subject (`sub`), audience (`aud`), expiration time (`exp`), etc. The signature is generated by signing the header and payload (usually with a secret key for HS256 or a private key for RS256).

- **Signing and Validation:** To “sign” a JWT, if using HMAC (HS256), the issuer uses a shared secret key. If using RSA/ECDSA (RS256, ES256), the issuer uses a private key. The signature ensures integrity – if the token is tampered with, verification fails. **Validation** of a JWT involves:

  1. Parsing the token and base64-decoding the parts.
  2. Checking the header to see what algorithm was used and perhaps a key ID (`kid`).
  3. Using the expected secret or public key to verify the signature against the header+payload. If this cryptographic check passes, the token is proven to be issued by a trusted party (assuming only the trusted issuer knows the secret or private key).
  4. Checking token claims for validity: e.g., ensure `exp` (expiry time) is in the future, `nbf` (not before) is in the past, and that audience or issuer matches what we expect. Kong’s JWT plugin, for instance, will reject tokens that are expired or not yet valid, and can be configured with acceptable issuers/audiences.
  5. If all checks pass, the token is considered **authenticated** and the claims can be trusted in handling authorization.

- **JWT Use in API Gateways:** The idea is that a user or client can obtain a JWT from an identity provider or auth server (e.g., via an OAuth2 authorization server, or some login service), then present that JWT on each request (usually in the `Authorization: Bearer <token>` header). The API gateway can validate the JWT on each request without needing to call back to the auth server (this is **stateless auth**). This is efficient and scalable: the gateway uses the public key or secret to verify the token and doesn’t need to store session state. JWTs are widely used for microservice auth because of this property. It’s important to use strong signing algorithms (asymmetric RSA/EC recommended over HS256 shared secrets in many cases).

- **Kong’s JWT Plugin Mechanics:** As noted earlier, Kong’s JWT plugin lets you declare one or more secrets or public keys for a Consumer, effectively declaring “this is how to verify tokens for this consumer.” When a request with a JWT comes in, Kong looks for the token in either:

  - The `Authorization` header (with schema Bearer by default),
  - A query parameter (if configured to allow, e.g. `?jwt=<token>`),
  - Or a cookie (if configured). By default, it checks header and query param names.

  Kong then tries each known key (the plugin maintains a list of keys associated with all Consumers) to verify the token’s signature. If one matches and verification passes, Kong knows which Consumer this token belongs to (because the credential used is tied to a Consumer). Kong can then inject headers to the upstream like `X-Consumer-Username` or `X-Consumer-Id` if needed, and apply any Consumer-specific plugins (like a rate limit per user). If verification fails or no token is provided when required, Kong returns `401 Unauthorized`. If multiple tokens are accidentally present (say in both header and query), Kong will reject the request to prevent ambiguity.

- **JWT in Our Demo Context:** We initially considered using Kong’s JWT plugin for auth, which would involve issuing JWTs to clients after they log in. For example, we could have had OAuth2 Proxy (or a custom step) mint a JWT for the user and have the client use that on subsequent calls. However, to keep things simpler, we relied on OAuth2 Proxy’s session cookie mechanism. JWT authentication is **not** directly used in the current flow (the access token from GitHub isn’t a JWT). We **skipped Kong’s JWT plugin** because OAuth2 Proxy already ensures the user is authenticated via cookie, and Kong isn’t inspecting that cookie by default.

However, it’s worth noting that if we used a provider like Google or Azure AD via OAuth2 Proxy, we might get an **ID Token JWT** in addition to an access token. In such a case, one could configure OAuth2 Proxy with `--set-authorization-header=true` to pass the JWT to Kong, and Kong’s JWT plugin (if configured with the correct Google public keys) could validate that on each request. This would be a form of defense-in-depth (Kong double-checks the token) but also somewhat redundant since OAuth2 Proxy already did the auth. More typically, you’d choose one approach: *either* have the gateway do JWT verification, *or* have an oauth2-proxy in front that manages login and sessions.

## Combining JWT and OAuth2 Proxy with Kong – Can They Work Together?

Given we have two authentication mechanisms (JWT auth and OAuth2 Proxy OAuth), a natural question is whether we should (or can) use them simultaneously with Kong. The answer depends on the use case, but generally they are used **alternatively** rather than fully together:

- **Either/Or Use Case:** In many scenarios, you would pick one approach. For instance:
  - If you have an SPA or mobile app that can handle tokens, you might use a pure JWT approach: The client gets a JWT from an identity service and then calls Kong directly with that token on each request. Kong’s JWT plugin secures the API in this case (no OAuth2 Proxy needed).
  - If you have web applications or need to leverage a third-party OAuth provider for login, using OAuth2 Proxy is convenient. It handles the user redirections and issues a session cookie. In this case the client (browser) isn’t sending a JWT on each request, it’s sending the cookie. Kong cannot validate that cookie itself (it doesn’t know how to decrypt it), so Kong must delegate auth to the proxy. Kong then doesn’t need its JWT plugin for that route.
- **Using Both for Different Clients:** It is possible to have a hybrid: for example, you might protect some routes with OAuth2 Proxy (for interactive user access via browser) and other routes with JWTs (for API-to-API communication or mobile clients). Kong could be configured with both, but you’d likely separate the paths or hosts. For instance, requests coming from a single-page app might include a JWT that Kong validates, whereas your web portal uses cookies. Combining these on the same route is tricky because Kong’s JWT plugin doesn’t know about the cookie from OAuth2 Proxy. One could conceive a scenario where after OAuth2 Proxy login, the proxy issues a JWT to the client (instead of or in addition to a cookie) and then Kong’s JWT plugin checks it. But that would require custom work (issuing JWTs in OAuth2 Proxy isn’t a standard feature unless the IdP provides one).
- **Chaining OAuth2 Proxy with Kong JWT:** If an identity provider provided JWTs, Kong could validate them. For example, if we used an OIDC provider that gives a JWT access token, and we set `--pass-access-token` and `--set-authorization-header`, the backend (or Kong) would see `Authorization: Bearer <JWT>`. We could then enable Kong’s JWT plugin on that route with the provider’s public key configured. Kong would then independently verify the JWT on every request, acting as a second layer. This ensures that even if someone bypassed OAuth2 Proxy (which is unlikely if network rules are set correctly) or if the token was stolen, Kong would still enforce validity. But in practice, since the OAuth2 Proxy is already in path, this duplication is usually unnecessary. Instead, one would rely on *either* the proxy’s session or have the proxy do the heavy lifting and just trust it.
- **Session vs Stateless:** OAuth2 Proxy by default uses **stateful sessions** (cookie with an encrypted blob or a session stored in an external store like Redis). JWT is stateless. Some prefer JWTs for their statelessness, but managing JWT issuance (refresh, revocation, etc.) can be complex too. OAuth2 Proxy gives a simple stateful model (with cookie refresh, you can logout by cookie invalidation, etc.). For internal APIs, stateless JWT auth is very popular. For user-facing web, OAuth2 Proxy is often simpler.
- **Kong’s OAuth2 Plugin or OIDC (Enterprise):** It’s worth noting Kong has its own OAuth2 authentication plugin and an OpenID Connect plugin (the latter in Kong Enterprise) which can handle the full login flow or token introspection. Those could eliminate the need for a separate OAuth2 Proxy, by having Kong directly interact with OAuth providers (like Cognito, Okta, etc.). In the open-source Kong, the JWT plugin plus an external identity provider is the common solution (with pre-issued JWTs). OAuth2 Proxy is effectively doing something similar externally.

**Bottom line:** In our demo, we chose **OAuth2 Proxy + Kong** and did not use JWT plugin concurrently, because the OAuth2 Proxy’s cookie-based auth was sufficient and easier for interactive login. We **could** enhance it by issuing a JWT to the client after login and having Kong validate it – but that would complicate the demo without much added benefit. For API-only clients (like test scripts or other services calling the API), one approach is to allow both cookie or JWT: e.g., if a request has a valid JWT, accept it; otherwise, if it has a session cookie, that’s fine too. Kong doesn’t support a plugin configuration like “JWT or cookie” out of the box (that logic would be custom), so often you’d run separate endpoints or an either-or check in the application layer.

To directly answer whether JWT and OAuth2 Proxy can be used **together** with Kong: **Yes**, but typically by dividing responsibility. OAuth2 Proxy can handle user login and then perhaps issue a JWT that Kong will honor for subsequent API calls. Without custom plugins, though, Kong can’t decrypt OAuth2 Proxy’s cookie on its own. So usually it’s one after the other (as we did: Kong passes to Proxy, Proxy authenticates then calls API). For most use-cases, using one mechanism at a time is sufficient – and adding both can be redundant. Our team chose to prioritize the OAuth2 Proxy approach for user-facing auth, and skip JWT in Kong for simplicityfile-vlrsohrq3l3b3xhshqpdxu.

## API Protection: Rate Limiting, Tracing, and Monitoring

Protecting an API is not only about authenticating users – it also involves controlling usage (rate limiting) and observing traffic (monitoring and tracing requests). Kong, as the gateway, provides capabilities for both, and Postman is used as a tool to test and demonstrate these features.

### Rate Limiting in Kong

Rate limiting is crucial to prevent abuse (like a client flooding the API with requests). We configured Kong’s **Rate Limiting plugin** on our API route. The configuration we used was a simple **fixed window limit** of 5 requests per minute per client (global limit)file-vlrsohrq3l3b3xhshqpdxu. Let’s clarify how this works:

- By default, without a Consumer or credential to distinguish clients, Kong’s rate limit plugin will use the client’s IP address to track limits (or it can be configured to use a specific header like an API key or the Consumer identifier). In our local test, calling from Postman/cURL, it’s all from one client anyway.

- The plugin tracks how many requests have been made in the current minute window. Kong adds HTTP headers to each response: `X-RateLimit-Limit-minute: 5` and `X-RateLimit-Remaining-minute: N` to let the client know the limit and remaining callsfile-vlrsohrq3l3b3xhshqpdxu. We observed these in Postman responses. On the 6th request within a minute, Kong responded with **HTTP 429 Too Many Requests**, indicating the limit was exceededfile-vlrsohrq3l3b3xhshqpdxu.

- We set `config.policy=local`, meaning the counter is node-localfile-vlrsohrq3l3b3xhshqpdxu. For a single Kong instance, that’s fine. In a multi-node AKS deployment of Kong, if we wanted a unified counter, we could use `policy=cluster` (which in Kong with a DB will use the shared datastore to count) or `policy=redis` to use an external Redis. Kong supports very flexible rate-limit strategies (even leaky bucket algorithms).

- The rate limiting plugin can also be applied **per consumer**. If we had the JWT plugin or key-auth identifying consumers, we could enforce, say, “5 requests/minute per user”. In our case, since we didn’t have Kong identify each end-user, we kept it global or per IP. However, if we wanted to ensure each logged-in user can make 5 requests/minute, we’d need Kong to recognize the user – which we could do if the user had a token or if we wrote a custom plugin to treat the `X-Forwarded-User` as a key. That’s an advanced scenario and wasn’t implemented. Instead, the demo just shows a generic protection.

- In terms of configuration, applying the plugin in Kubernetes can be done by a declarative YAML (KongPlugin resource) as shown below (conceptually):

  ```yam;
  apiVersion: configuration.konghq.com/v1
  kind: KongPlugin
  metadata:
    name: demo-rate-limit
    namespace: demo
  plugin: rate-limiting
  config:
    minute: 5
    policy: local
  ```

  Then in the Ingress definition for the API, an annotation `konghq.com/plugins: demo-rate-limit` attaches it. When Kong’s controller sees this, it enables that plugin on the corresponding route. We could also have applied it via Kong’s Admin API as our instructions did for the Docker Compose case (POST to `/routes/{id}/plugins` with the config)file-vlrsohrq3l3b3xhshqpdxu.

In AKS, the same configuration works. We might also integrate Kong’s rate limiting metrics with monitoring – e.g., Kong can emit logs or metrics when limits are hit. For instance, Kong’s Prometheus plugin (if enabled) would expose counters for HTTP status codes, so you could see the count of 429s, etc.

### Request Tracing and Monitoring with Postman

For demonstration and troubleshooting, we used **Postman** to simulate client requests and observe the system’s behavior. Here’s how Postman (or any HTTP client) aids in tracing and monitoring the request flow:

- **OAuth Flow Visibility:** By using Postman to hit the API initially, we can see the *redirect* response from Kong/OAuth2 Proxy. In our test, a request to `/api/hello` without a session got a `302 Found` redirect to GitHub (something like `Location: https://github.com/login/oauth/authorize?...`). Postman’s console or output shows this redirect. While Postman itself won’t follow the OAuth login (since it’s not a browser), this confirms the gateway and proxy are correctly requiring auth. In a browser, this would seamlessly go to GitHub.
- **Simulating Authenticated Calls:** After obtaining the session cookie (by actually logging in via a browser, or by capturing the cookie set by OAuth2 Proxy), we can include that cookie in Postman for subsequent requests. We did this to test a successful authenticated call. Postman then directly requests `/api/hello` with the cookie header, and we observed a `200 OK` with the JSON response containing our user info and message. This shows the flow after login is working – OAuth2 Proxy recognized the cookie and allowed the request through to the backend.
- **Headers and Debug Info:** Postman allows us to inspect all response headers. We used this to verify the presence of rate-limit headers from Kong and the forwarded user headers reaching the backend’s response. For example, in the 200 OK response, we saw headers like `X-RateLimit-Remaining` and in the body the `user` field was populated – confirming integration. This manual tracing is helpful for the team to understand which component added what header (Kong adds rate-limit headers, OAuth2 Proxy adds X-Forwarded-User).
- **Rate Limit Testing:** We also used Postman (or a simple curl loop) to send multiple requests in quick succession. The first 5 returned 200 OK, and the 6th returned 429, as expectedfile-vlrsohrq3l3b3xhshqpdxu. Postman’s interface made it easy to send requests one after another. This demonstrated the rate limiting in action to the team.
- **Monitoring Tools:** In a more advanced sense, “monitoring” could refer to setting up Postman Monitors – which are automated collections of requests run on a schedule to test your API’s availability. One could create a Postman collection for the API (with proper authentication) and have it run periodically to ensure everything is up and to measure response times. For our demo, we didn’t set up an automated monitor, but it’s something to mention: Postman can hit the endpoint and verify responses (e.g., status code, content) regularly, which is useful for uptime monitoring.
- **Kong Manager / Logging:** Since Kong OSS doesn’t include a full UI, we relied on logs. If you run Kong locally, it logs each request with details (method, URL, response code, etc.). In Kubernetes, you can check the Kong proxy pod’s logs. We did check logs to confirm that Kong was invoking the correct route and plugin. For instance, Kong log shows HTTP 429 for the rate-limited request, verifying the plugin worked. The logs also show upstream latency (time taken by OAuth2 Proxy and backend) and total latency, which is useful for performance monitoring.
  - Kong Enterprise’s **Manager** UI or Kong Konnect could give a graphical view of the service, but in our case we likely did not use those due to using Kong OSS. Instead, simple logging and manual monitoring sufficed. We listed “Kong Manager / Logs” in the plan for monitoring, to indicate either using an interface or logs to observe traffic.
- **Tracing (Distributed Tracing):** While not implemented in this demo, it’s worth noting that Kong can integrate with tracing systems (like Jaeger or Zipkin) by adding a plugin that injects trace IDs. If we had that, each request through Kong could be traced across services. Our mention of “request tracing” was more in the sense of following the path of a request manually. For teaching purposes, we stepped through each component to trace how a request travels (client -> Kong -> oauth2 -> backend, and back).

Using Postman and the logs, the team can clearly see the effects of our protections:

1. **Unauthorized request** -> **redirect** (no data leaked, secure).
2. **Authorized request** -> **success** (200 OK with user info).
3. **Excessive requests** -> **rate limit** (429 responses after threshold).
4. Headers like `X-Forwarded-User` present in backend’s output indicate identity propagation working.

These verifications give confidence that our configuration is correct. We also emphasize to the team that any 401/403 errors during setup likely mean a misconfiguration in OAuth2 Proxy (e.g., cookie or client ID issues), and any missing rate limit headers mean the plugin might not be properly applied – demonstrating how to use these tools for troubleshooting.

## Kong vs. Tyk: Core Feature Comparison

Finally, let’s **briefly compare Kong and Tyk** (another popular open-source API gateway) in terms of the core features relevant to our discussion: plugin support, policy enforcement, authentication options, scalability, and monitoring.

- **Plugin Support and Extensibility:** Kong is known for its rich ecosystem of plugins. As of writing, Kong offers dozens of plugins (77+ in open-source) for various functionality – authentication (basic auth, key auth, JWT, OAuth2, etc.), security (IP restriction, bot detection), traffic control (rate limiting, caching), transformations (request/response modification), logging (to files, HTTP, syslog), and more. Kong is built on Nginx with Lua via OpenResty, so advanced users can even write custom Lua plugins for bespoke needs. (Kong’s plugin development kit makes this easier). Some advanced plugins (LDAP auth, OpenID Connect) are reserved for Kong Enterprise, but the OSS selection is still very broad.
   Tyk, in its open-source gateway, has fewer built-in plugins (on the order of <10 main plugins). It covers basic needs (auth, quota, rate limit, etc.), but it’s not as extensive as Kong’s hub. However, Tyk is written in Go and provides a flexible plugin system supporting multiple languages: you can write custom plugins in JavaScript, Python, Lua, .NET, or Go, and even via gRPC sidecar plugins. This polyglot plugin support is a strength of Tyk – it might be easier to integrate custom logic in a language of your choice, whereas Kong (OSS) limits you to Lua (unless you use Kong’s PDK which is Lua-based or go with enterprise/go plugins in Kong 2.x+). In summary, Kong has more pre-made plugins (community-driven), while Tyk has a simpler core set but the ability to extend in various languages.
- **Policy Enforcement (Rate limiting, Quotas, etc.):** Both Kong and Tyk are capable of enforcing policies like rate limiting, quotas, and access control.
  - In Kong, as we’ve seen, rate limiting is a plugin that can be configured per service, route, consumer, or globally. Kong’s rate limiting can work with a database or Redis to handle cluster-wide limits. It’s very production-proven and can handle high load. Kong also has plugins for IP whitelisting/blacklisting and even integration with external policy engines (OPA, etc., via plugins).
  - Tyk also provides rate limiting and quota management. In Tyk, policies are often defined in its API definitions or via its dashboard. Tyk’s gateway has a concept of “policies” which can bundle an API’s rate limits, quotas, and access rules, and then you can assign keys or tokens to those policies. Tyk’s open source supports rate limiting (it typically requires Redis for storing counters in a cluster). According to some benchmarks, Kong’s rate limiting (with Redis) vs Tyk’s are comparable, but Tyk highlights an algorithm for distributed rate limiting without single Redis bottleneck in newer versions. Both can enforce per-user or global limits similarly. So on policy enforcement, they are roughly equivalent in capabilities, though the configuration differs (Kong via plugins/consumers, Tyk via policy docs or the dashboard).
- **Authentication Features:** Both gateways support a range of auth methods:
  - **Kong:** Key auth (API keys), Basic auth, JWT, OAuth2 (acting as an OAuth server for issuing tokens), HMAC signatures, and others via plugins. For OIDC integration, Kong OSS might require some setup (e.g., using the JWT plugin with OIDC provider’s keys). The Kong Enterprise edition has a turnkey OIDC plugin that can handle the full flow with providers. The TechTarget analysis notes Kong OSS has slightly more limited auth options out-of-the-box for complex scenarios like LDAP or OIDC (those might need enterprise plugins). But in practice, Kong OSS covers most needs except direct integration with external OAuth servers (which we solved by using OAuth2 Proxy in our demo).
  - **Tyk:** Tyk supports similar methods – it has API keys, basic auth, JWT validation, OAuth2 (Tyk can do OAuth2 token issuance and validation), and it touts support for OpenID Connect and mTLS in the open source gateway. Tyk’s docs claim you don’t need extra plugins for things like OIDC or mTLS – they’re built-in. In other words, Tyk comes with a wide range of auth support “out of the box” without needing to write custom plugins, whereas with Kong OSS, something like OIDC might need custom configuration or the Enterprise plugin. In our context (GitHub OAuth), neither Kong nor Tyk have a direct built-in integration, so we used OAuth2 Proxy – we could similarly pair OAuth2 Proxy with Tyk if needed. Tyk’s “Authentication that suits your setup” claim is that whether you need modern OAuth/OIDC or old-school Basic Auth, Tyk can handle it in OSS. Kong can handle them too, but maybe requires more community plugin usage for some.
- **Performance and Scalability:** Both Kong and Tyk are high-performance gateways designed to scale horizontally.
  - Kong is built on Nginx (which is very efficient at proxying). Benchmarks have shown Kong to have excellent throughput and lower latency under high concurrency. In fact, Kong tends to **surpass Tyk in raw performance benchmarks** for throughput and latency at high request rates. Kong’s architecture (with Nginx + C* or Postgres + local caching) handles thousands of requests per second easily. One cited test found Kong handled concurrency and scaling “more efficiently,” which is important when spikes occur. Kong nodes are stateless (if DB-less or if DB-backed with caching), so you can scale by adding more pods/nodes behind a load balancer. The main consideration is if using a database, that database can become a bottleneck or single point of failure. For HA, you’d run a cluster of Postgres/Cassandra for Kong. Kong’s DB-less mode avoids that by using declarative config – great for Kubernetes (we can treat Kong config as code).
  - Tyk is written in Go and also very performant. Tyk easily handles thousands of requests per second too (in one test, ~4000 rps with default settings). Tyk’s scalability notes: it uses Redis as a backing store for some data (and MongoDB for analytics in older setups; newer versions can be configured differently). To scale Tyk, you add more gateway instances connected to the same Redis/Mongo. Tyk tends to be more CPU-bound; it’s noted that once CPU usage gets high, you should scale out another node. Kong’s bottleneck is often the database or network I/O, whereas Tyk’s in-memory operations might need tuning. In short, both scale horizontally and can be used in large distributed systems. Kong might have an edge in peak performance, whereas Tyk is perfectly fine for most loads and has improved its performance over versions (especially if one disables certain features or uses their async logging).
  - From our demo perspective (a handful of requests), both are overkill in capacity. But it’s good to know that if this were a production API, Kong would not likely be the limiting factor – it can handle a classroom of users easily, and an enterprise’s worth with proper setup.
- **Monitoring and Analytics:**
  - **Kong:** In OSS, Kong provides logging (you can configure file logs, HTTP logs to an external system, etc.) and metrics (via plugins like the Prometheus plugin). You can plug Kong into Grafana/Prometheus to monitor request rates, latency, etc. Kong’s Admin API also allows querying some stats and its status. For full analytics (e.g., historical charts of request counts, top APIs, etc.), Kong offers an enterprise solution (Kong Manager + Kong Vitals) which is not available in OSS. In our demo, we stuck to basic monitoring: checking logs and using Postman to watch responses. But one could integrate Kong with ELK stack or other monitoring. Kong is very ops-friendly in that sense but you have to set up the tooling.
  - **Tyk:** Tyk open source gateway has a component called the “Pump” in Tyk’s architecture. The Pump takes API analytics data (request metrics, logs) from the gateway and pushes it to various sinks (InfluxDB, Prometheus, Elastic, etc.). The open source gateway will emit raw data and the Pump (also open source) can store it. However, the visualization and analysis of that data is typically done via the **Tyk Dashboard**, which is a commercial component (closed-source) unless you use their free developer license. In purely open source usage, you might integrate the Pump with something like Grafana to see metrics. Tyk does have a “Developer Portal” and dashboard in their closed source offering which gives a lot of out-of-the-box analytics (like charts of usage by API, by key, etc.). So for monitoring, both gateways rely on external systems in OSS form (Prometheus/Grafana, ELK, etc.). Kong Enterprise has it built-in, Tyk has an official paid dashboard, whereas Kong OSS might require third-party UIs (there are community ones like Konga for basic management, but not as advanced for analytics).
- **Community and Support:** (Not explicitly asked, but notable) Kong has a large community and many community plugins, as it’s very open and extensible. Tyk is open source but more driven by the company (slightly smaller community footprint). The TechTarget summary put it nicely: *“Kong is more community-driven and extensible, while Tyk focuses on operational simplicity”*. Tyk’s benefit is a cohesive design where many features (like OIDC) just work out-of-box in OSS, so less need to hunt for plugins.

In conclusion, **Kong vs Tyk** – both are capable API gateways. Kong often wins in raw performance and breadth of plugin functionality. Tyk, however, provides a strong open-source package with many features included and flexible plugin development, and it’s often praised for ease of setup. In our demo’s context (GitHub OAuth, JWT, rate limiting), either gateway could achieve the goal. We chose Kong due to familiarity and its integration as an ingress controller in K8s (Tyk can also act in K8s, but Kong’s ingress controller is very straightforward to deploy with Helm). The comparison above was to help the team understand that the core concepts (auth, rate limiting, monitoring) apply to any gateway, with some differences in implementation.

Both Kong and Tyk are horizontally scalable and suitable for production. Kong’s slight edge in community plugins and proven high-concurrency performance made it a solid choice for our needs, but teams should consider specific requirements (e.g., need built-in OIDC? Or prefer a particular management UI?) when choosing between them.

## Conclusion

This report outlined a comprehensive architecture for API security using Kong Gateway and OAuth2 Proxy. We covered how to deploy and configure the system in both local Kubernetes (Minikube) and Azure AKS, how Kong acts as an ingress API gateway enforcing rate limits and routing rules, and how OAuth2 Proxy seamlessly adds OAuth authentication (with GitHub in our case) on top of the API without changing the backend. We explained the major configuration items that make this possible – from Kong’s plugin settings (e.g., rate limit thresholds, JWT credentials) to OAuth2 Proxy’s OAuth client setup and cookie handling – and traced the lifecycle of a request from an unauthenticated redirect to an authenticated API response.

Additionally, we provided a technical overview of JWTs to solidify understanding of token-based auth, and we discussed how JWT auth and the OAuth2 Proxy approach might be combined or used separately with Kong. Using Postman and logs, we illustrated how to monitor and verify that the protections are working (seeing 302 redirects for login, 200 OK for authorized calls, and 429 errors when limits are exceeded). Finally, we compared Kong with Tyk to give context on gateway choices, noting that both are capable but Kong offers more plugins and community support while Tyk emphasizes all-in-one simplicity and multi-language extensibility.

Armed with this knowledge, our team should be comfortable with the demo architecture and confident in presenting how API gateways like Kong secure and manage microservice APIs. This setup demonstrates a practical, centralized way to enforce **authentication, rate limiting, and monitoring** – critical aspects of API management – in both development and production environments. By using industry-standard tools (Kong, OAuth2 Proxy, JWTs) and protocols (OAuth2/OIDC, JWT), we ensure our API can be protected in a robust and scalable manner, whether running locally or in the cloud.